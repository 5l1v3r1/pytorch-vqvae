{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder,MNIST\n",
    "import os\n",
    "from torchvision.transforms import Compose,Normalize,Resize,ToTensor\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from modules import VectorQuantizedVAE, ResBlock, weights_init, AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2D(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args,**kwargs):\n",
    "        super(MaskedConv2D,self).__init__(*args,**kwargs)\n",
    "        self.mask_type = mask_type\n",
    "        _,_,h,w = self.weight.size()\n",
    "        self.mask = torch.zeros((h,w)).cuda()\n",
    "        self.mask[:h //2,:] = 1.\n",
    "        self.mask[h//2,:w//2] = 1.\n",
    "        if mask_type == \"B\":\n",
    "            self.mask[h//2,w//2] = 1.\n",
    "    def forward(self,x):\n",
    "        self.weight.data = self.weight.data * self.mask\n",
    "        return super(MaskedConv2D,self).forward(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = MaskedConv2D(\"A\",1,16,5)\n",
    "\n",
    "# x = torch.randint(low=0,high=255,size=(8,1,32,32)).long()\n",
    "# m(x.float())\n",
    "# print(m.mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self,num_layers=15, filt_size=5,inp_channels=1,num_fm=16):\n",
    "        super(PixelCNN,self).__init__()\n",
    "        first_layer = MaskedConv2D(\"A\",inp_channels,num_fm,filt_size,padding=filt_size//2,bias=False) # padding to get same conv\n",
    "        layers = [first_layer]\n",
    "        for i in range(num_layers-1):\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(MaskedConv2D(\"B\",num_fm,num_fm,filt_size,padding=filt_size//2,bias=False))\n",
    "        # 1x 1 conv to get logits\n",
    "        last_layer = nn.Conv2d(num_fm,256,1)\n",
    "        layers.append(last_layer)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # no softmax cuz it gets combined in loss\n",
    "        return self.layers(x)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr = DataLoader(MNIST('../data', train=True, download=True, transform=ToTensor()),\n",
    "#                      batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "# pcnn = PixelCNN().cuda()\n",
    "# h,w = 28,28\n",
    "\n",
    "# opt = Adam(pcnn.parameters(),lr=0.01)\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     t = tqdm.tqdm(tr)\n",
    "#     for x,y in t:\n",
    "#         opt.zero_grad()\n",
    "#         x = x.cuda()\n",
    "#         x_tild = pcnn(x)\n",
    "#         loss= F.cross_entropy(x_tild,x.long().squeeze())\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         t.set_description(\"epoch:%iloss:%6.4f\"%(epoch,float(loss.data)))\n",
    "\n",
    "# # x_tild = p(x.float())\n",
    "# # x.size()\n",
    "\n",
    "# # sampling\n",
    "# sample = torch.zeros((1,1,h,w)).cuda()\n",
    "\n",
    "\n",
    "# for i in range(h):\n",
    "#     for j in range(w):\n",
    "#         s_tild = pcnn(sample)\n",
    "#         probs = F.softmax(s_tild,dim=1)[0,:,i,j]\n",
    "#         sample[:,0,i,j] = torch.multinomial(probs,1)\n",
    "        \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.imshow(sample[0,0],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMaskedConv2D(nn.Conv2d):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super(VMaskedConv2D,self).__init__(*args,**kwargs)\n",
    "        _,_,h,w = self.weight.size()\n",
    "        self.mask = torch.zeros((h,w)).cuda()\n",
    "        self.mask[:h //2,:] = 1.\n",
    "    def forward(self,x):\n",
    "        self.weight.data = self.weight.data * self.mask\n",
    "        return super(VMaskedConv2D,self).forward(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vm = VMaskedConv2D(1,16,5)\n",
    "\n",
    "# x = torch.randint(low=0,high=255,size=(8,1,32,32)).long()\n",
    "#print(vm.mask)\n",
    "# vm(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMaskedConv2D(nn.Conv2d):\n",
    "    def __init__(self,mask_type,*args,**kwargs):\n",
    "        super(HMaskedConv2D,self).__init__(*args,**kwargs)\n",
    "        _,_,h,w = self.weight.size()\n",
    "        self.mask = torch.zeros((h,w)).cuda()\n",
    "        self.mask[h //2,:w//2] = 1.\n",
    "        if mask_type == \"B\":\n",
    "            self.mask[h //2,:w//2+1] = 1.\n",
    "            \n",
    "    def forward(self,x):\n",
    "        self.weight.data = self.weight.data * self.mask\n",
    "        return super(HMaskedConv2D,self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hm = HMaskedConv2D(\"A\",1,16,5)\n",
    "\n",
    "# hm.mask\n",
    "\n",
    "#x = torch.randint(low=0,high=255,size=(8,1,32,32))\n",
    "\n",
    "#hm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLayer(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,filter_size,mask_type,cond_size,skip_cxn=False):\n",
    "        super(GatedLayer,self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.vconv = VMaskedConv2D(in_channels,2*out_channels,filter_size,padding=filter_size//2,bias=False)\n",
    "        self.hconv = HMaskedConv2D(mask_type,in_channels,2*out_channels,filter_size,padding=filter_size//2,bias=False)\n",
    "        self.cond_embed = nn.Linear(cond_size, 2*out_channels)\n",
    "        self.v2h_conv1x1 = nn.Conv2d(2*out_channels,2*out_channels,1)\n",
    "        self.skip_conv1x1 = nn.Conv2d(out_channels,out_channels,1)\n",
    "        self.skip_cxn = skip_cxn\n",
    "    def forward(self,x_v,x_h,y):\n",
    "        #y is the conditioning\n",
    "        vc2p = self.vconv(x_v)\n",
    "        vcf, vcg = torch.split(vc2p,self.out_channels,dim=1)\n",
    "        y = self.cond_embed(y)\n",
    "        yf,yg = torch.split(y,self.out_channels,dim=1)\n",
    "        #broadcast conditioning vector to all pixels\n",
    "        fv = F.tanh(vcf + yf[:,:,None,None] )\n",
    "        gv = F.sigmoid(vcg + yg[:,:,None,None] )\n",
    "        vout = fv * gv\n",
    "        \n",
    "        \n",
    "        hc2p = self.hconv(x_h) + self.v2h_conv1x1(vc2p)\n",
    "        hcf, hcg = torch.split(hc2p,self.out_channels,dim=1)\n",
    "        fh = F.tanh(hcf + yf[:,:,None,None] )\n",
    "        gh = F.sigmoid(hcg + yg[:,:,None,None] )\n",
    "        hout = fh * gh\n",
    "        if self.skip_cxn:\n",
    "            hout  = self.skip_conv1x1(hout) + x_h\n",
    "            \n",
    "        return vout,hout\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gl = GatedLayer(1,8,5,\"A\",3,skip_cxn=True)\n",
    "\n",
    "# x = torch.randint(low=0,high=255,size=(8,1,32,32))\n",
    "\n",
    "# y = torch.randint(low=0,high=512,size=(8,3))\n",
    "\n",
    "# v,h = gl(x,x,y)\n",
    "\n",
    "# gl2 = GatedLayer(8,8,5,\"B\",3,skip_cxn=True)\n",
    "\n",
    "# gl2(v,h,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedPixelCNN(nn.Module):\n",
    "    def __init__(self,num_layers=15, filt_size=5,in_channels=1,\n",
    "                 num_fm=16,cond_size=3,im_range=256):\n",
    "        \n",
    "        super(GatedPixelCNN,self).__init__()\n",
    "        self.first_layer = GatedLayer(in_channels=in_channels,out_channels=num_fm,\n",
    "                                 filter_size=filt_size,mask_type=\"A\",cond_size=3,skip_cxn=True)\n",
    "        layers = []\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.append(GatedLayer(in_channels=num_fm,out_channels=num_fm,\n",
    "                                 filter_size=filt_size,mask_type=\"B\",cond_size=3,skip_cxn=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "           \n",
    "        self.last_layer = nn.Conv2d(num_fm,im_range,1)\n",
    "        \n",
    "        #self.apply(weights_init)\n",
    "            \n",
    "    def forward(self,x,y):\n",
    "        xv,xh = self.first_layer(x,x,y)\n",
    "        for layer in self.layers:\n",
    "            xv,xh = layer(xv,xh,y)\n",
    "        return self.last_layer(xh)\n",
    "    \n",
    "    \n",
    "    def sample(self, y, shape=(8, 8), batch_size=64):\n",
    "        x = torch.zeros((batch_size,1, *shape),\n",
    "            dtype=torch.int64).float().cuda()\n",
    "\n",
    "        h,w = shape\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                # calc unnormalized p(x_ij|x<) logits\n",
    "                pxij_logit = self.forward(x,y)[:,:,i,j]\n",
    "\n",
    "                # compute normalized p(x_ij|x<) \n",
    "                pxij = F.softmax(pxij_logit,dim=1)\n",
    "                # sample from p(x_ij|x<)\n",
    "                x[:,0,i,j].copy_(torch.multinomial(pxij,1).squeeze())\n",
    "        return x.type(torch.int64)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randint(low=0,high=512,size=(8,1,21,21))\n",
    "\n",
    "# y = torch.randint(low=0,high=512,size=(8,3))\n",
    "\n",
    "# gpcnn = GatedPixelCNN(im_range=512)\n",
    "\n",
    "# #gpcnn(x,y)\n",
    "\n",
    "# gpcnn = GatedPixelCNN(im_range=512).cuda()\n",
    "\n",
    "# h,w = 21,21\n",
    "\n",
    "# opt = Adam(gpcnn.parameters(),lr=0.001)\n",
    "# xs = torch.randint(low=0,high=512,size=(10,8,1,21,21)).cuda()\n",
    "# ys = torch.randint(low=0,high=512,size=(10,8,3)).cuda()\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     losses = []\n",
    "#     for i in range(10):\n",
    "#         x = xs[i]\n",
    "#         y = ys[i]\n",
    "\n",
    "#         opt.zero_grad()\n",
    "#         x_tild = gpcnn(x,y)\n",
    "#         loss= F.cross_entropy(x_tild,x.long().squeeze())\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         losses.append(float(loss.data))\n",
    "#     print(np.mean(losses))\n",
    "\n",
    "# sample = torch.zeros((1,1,h,w)).cuda()\n",
    "# y = ys[0][0][None,:]\n",
    "# for i in range(h):\n",
    "#     for j in range(w):\n",
    "#         # calc unnormalized p(x_ij|x<) logits\n",
    "#         pxij_logit = gpcnn(sample,y)[0,:,i,j]\n",
    "        \n",
    "#         # compute normalized p(x_ij|x<) \n",
    "#         pxij = F.softmax(pxij_logit,dim=0)\n",
    "#         # sample from p(x_ij|x<)\n",
    "#         sample[:,0,i,j] = torch.multinomial(pxij,1)\n",
    "        \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.imshow(sample[0,0],cmap=\"jet\")\n",
    "\n",
    "# x = torch.randint(low=0,high=512,size=(8,1,21,21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the 4th image experiment\n",
    "# this will encode the 21x21x1 from bigger VQ-VAE and encode to 3x1 ze, which is then made discrete thru VQ operation with 3 separate tables\n",
    "# then autoregressive decoder will decode back to 21x21 discrete latents, which then are looked up and passed thru deconv decoder\n",
    "# of bigger autoencoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim, hidden_dim,final_dim):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.final_dim = final_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=4, stride=2, padding=1),\n",
    "            ResBlock(hidden_dim),\n",
    "            ResBlock(hidden_dim),\n",
    "            nn.Conv2d(in_channels=hidden_dim, out_channels=final_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.AvgPool2d(5)\n",
    "            )\n",
    "        self.apply(weights_init)\n",
    "    def forward(self,x):\n",
    "        ze = self.encoder(x)\n",
    "        \n",
    "        return ze.view(-1,self.final_dim,1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ(nn.Module):\n",
    "    def __init__(self, K=512,D=3):\n",
    "        super(VQ,self).__init__()\n",
    "        self.e =  torch.randn(size=(K,D)).float().cuda()\n",
    "        \n",
    "    def forward(self,ze):\n",
    "        l2diff = (ze.transpose(2,1) - self.e[None,:])**2\n",
    "        z = l2diff.argmin(1) # discrete\n",
    "        zq = torch.cat((self.e[z[:,0],0,None], self.e[z[:,1],1,None], self.e[z[:,2],2,None]),dim=-1) # continuous\n",
    "        return z,zq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelVQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelVQVAE,self).__init__()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "VQVAE = AutoEncoder(3, 256).cuda()\n",
    "VQVAE.load_state_dict(torch.load(\"./models/vqvae_dataset=Boxing-v0_lr=0.0003_K=512_dim=256_lambda_=1_decoder=deconv_batch_size=32_epochs=100_num_workers=4_num_frames=60000_resize_to=84/Boxing-v0_autoencoder.pt\"))\n",
    "\n",
    "vqe = Encoder(256,16,3).cuda()\n",
    "vq = VQ().cuda()\n",
    "decoder = GatedPixelCNN(im_range=512,cond_size=z.size(1)).cuda()\n",
    "\n",
    "\n",
    "\n",
    "x = torch.randint(0,255,(batch_size,3,84,84)).cuda()\n",
    "\n",
    "Z, ZE = VQVAE.encode(x)\n",
    "\n",
    "ZE.size()\n",
    "\n",
    "ze = vqe(ZE)\n",
    "z,zq = vq(ze)\n",
    "zq.retain_grad()\n",
    "Z_tild = decoder(Z[:,None].float(),zq)\n",
    "rec_loss= F.cross_entropy(Z_tild,Z.long())\n",
    "rec_loss.backward(retain_graph=True)\n",
    "z_e.backward(zq.grad, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train():\n",
    "    train_loss = []\n",
    "    #for batch_idx, (x, _) in enumerate(train_loader):\n",
    "    x = torch.randint(0,255,(batch_size,3,84,84)).cuda()\n",
    "    do_update = True #batch_idx % update_freq == 0\n",
    "    start_time = time.time()\n",
    "    x = x.to(DEVICE)\n",
    "\n",
    "    if do_update:\n",
    "        opt.zero_grad()\n",
    "\n",
    "    Z, ZE = VQVAE.encode(x)\n",
    "    ze = vqe(ZE)\n",
    "    z,zq = vq(ze)\n",
    "    Z_tild = decoder(Z[:,None].float(),zq)\n",
    "\n",
    "    zq.retain_grad()\n",
    "\n",
    "    # PixelCNN loss\n",
    "    loss_recons = F.cross_entropy(Z_tild,Z.long())\n",
    "    loss_recons.backward(retain_graph=True)\n",
    "\n",
    "    # Straight-through estimator\n",
    "    ze.backward(zq.grad, retain_graph=True)\n",
    "\n",
    "    # Vector quantization objective\n",
    "    if do_update:\n",
    "        vq.embedding.zero_grad()\n",
    "    loss_vq = F.mse_loss(zq, ze.detach())\n",
    "    loss_vq.backward(retain_graph=True)\n",
    "\n",
    "    # Commitment objective\n",
    "    loss_commit = args.lambda_ * F.mse_loss(ze, zq.detach())\n",
    "    loss_commit.backward()\n",
    "    if do_update and batch_idx != 0:\n",
    "        opt.step()\n",
    "\n",
    "    train_loss.append(to_scalar([loss_recons, loss_vq]))\n",
    "\n",
    "    if (batch_idx + 1) % PRINT_INTERVAL == 0:\n",
    "        print('\\tIter [{}/{} ({:.0f}%)]\\tLoss: {} Time: {}'.format(\n",
    "            batch_idx * len(x), len(train_loader.dataset),\n",
    "            PRINT_INTERVAL * batch_idx / len(train_loader),\n",
    "            np.asarray(train_loss)[-PRINT_INTERVAL:].mean(0),\n",
    "            time.time() - start_time\n",
    "        ))\n",
    "\n",
    "return np.mean(np.asarray(train_loss),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 21, 21])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
