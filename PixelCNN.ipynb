{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder,MNIST\n",
    "import os\n",
    "from torchvision.transforms import Compose,Normalize,Resize,ToTensor\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2D(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args,**kwargs):\n",
    "        super(MaskedConv2D,self).__init__(*args,**kwargs)\n",
    "        self.mask_type = mask_type\n",
    "        _,_,h,w = self.weight.size()\n",
    "        self.mask = torch.zeros((h,w)).cuda()\n",
    "        self.mask[:h //2,:] = 1.\n",
    "        self.mask[h//2,:w//2] = 1.\n",
    "        if mask_type == \"B\":\n",
    "            self.mask[h//2,w//2] = 1.\n",
    "    def forward(self,x):\n",
    "        self.weight.data = self.weight.data * self.mask\n",
    "        return super(MaskedConv2D,self).forward(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = MaskedConv2D(\"A\",1,16,5)\n",
    "\n",
    "# x = torch.randint(low=0,high=255,size=(8,1,32,32)).long()\n",
    "# m(x.float())\n",
    "# print(m.mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self,num_layers=15, filt_size=5,inp_channels=1,num_fm=16):\n",
    "        super(PixelCNN,self).__init__()\n",
    "        first_layer = MaskedConv2D(\"A\",inp_channels,num_fm,filt_size,padding=filt_size//2,bias=False) # padding to get same conv\n",
    "        layers = [first_layer]\n",
    "        for i in range(num_layers-1):\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(MaskedConv2D(\"B\",num_fm,num_fm,filt_size,padding=filt_size//2,bias=False))\n",
    "        # 1x 1 conv to get logits\n",
    "        last_layer = nn.Conv2d(num_fm,256,1)\n",
    "        layers.append(last_layer)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # no softmax cuz it gets combined in loss\n",
    "        return self.layers(x)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr = DataLoader(MNIST('../data', train=True, download=True, transform=ToTensor()),\n",
    "#                      batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "# pcnn = PixelCNN().cuda()\n",
    "# h,w = 28,28\n",
    "\n",
    "# opt = Adam(pcnn.parameters(),lr=0.01)\n",
    "\n",
    "# for epoch in range(5):\n",
    "#     t = tqdm.tqdm(tr)\n",
    "#     for x,y in t:\n",
    "#         opt.zero_grad()\n",
    "#         x = x.cuda()\n",
    "#         x_tild = pcnn(x)\n",
    "#         loss= F.cross_entropy(x_tild,x.long().squeeze())\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         t.set_description(\"epoch:%iloss:%6.4f\"%(epoch,float(loss.data)))\n",
    "\n",
    "# # x_tild = p(x.float())\n",
    "# # x.size()\n",
    "\n",
    "# # sampling\n",
    "# sample = torch.zeros((1,1,h,w)).cuda()\n",
    "\n",
    "\n",
    "# for i in range(h):\n",
    "#     for j in range(w):\n",
    "#         s_tild = pcnn(sample)\n",
    "#         probs = F.softmax(s_tild,dim=1)[0,:,i,j]\n",
    "#         sample[:,0,i,j] = torch.multinomial(probs,1)\n",
    "        \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# plt.imshow(sample[0,0],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMaskedConv2D(nn.Conv2d):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super(VMaskedConv2D,self).__init__(*args,**kwargs)\n",
    "        _,_,h,w = self.weight.size()\n",
    "        self.mask = torch.zeros((h,w)).cuda()\n",
    "        self.mask[:h //2,:] = 1.\n",
    "    def forward(self,x):\n",
    "        self.weight.data = self.weight.data * self.mask\n",
    "        return super(VMaskedConv2D,self).forward(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vm = VMaskedConv2D(1,16,5)\n",
    "\n",
    "# x = torch.randint(low=0,high=255,size=(8,1,32,32)).long()\n",
    "#print(vm.mask)\n",
    "# vm(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMaskedConv2D(nn.Conv2d):\n",
    "    def __init__(self,mask_type,*args,**kwargs):\n",
    "        super(HMaskedConv2D,self).__init__(*args,**kwargs)\n",
    "        _,_,h,w = self.weight.size()\n",
    "        self.mask = torch.zeros((h,w)).cuda()\n",
    "        self.mask[h //2,:w//2] = 1.\n",
    "        if mask_type == \"B\":\n",
    "            self.mask[h //2,:w//2+1] = 1.\n",
    "            \n",
    "    def forward(self,x):\n",
    "        self.weight.data = self.weight.data * self.mask\n",
    "        return super(HMaskedConv2D,self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hm = HMaskedConv2D(\"A\",1,16,5)\n",
    "\n",
    "# hm.mask\n",
    "\n",
    "#x = torch.randint(low=0,high=255,size=(8,1,32,32))\n",
    "\n",
    "#hm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLayer(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,filter_size,mask_type,cond_size,skip_cxn=False):\n",
    "        super(GatedLayer,self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.vconv = VMaskedConv2D(in_channels,2*out_channels,filter_size,padding=filter_size//2,bias=False)\n",
    "        self.hconv = HMaskedConv2D(mask_type,in_channels,2*out_channels,filter_size,padding=filter_size//2,bias=False)\n",
    "        self.cond_embed = nn.Linear(cond_size, 2*out_channels)\n",
    "        self.v2h_conv1x1 = nn.Conv2d(2*out_channels,2*out_channels,1)\n",
    "        self.skip_conv1x1 = nn.Conv2d(out_channels,out_channels,1)\n",
    "        self.skip_cxn = skip_cxn\n",
    "    def forward(self,x_v,x_h,y):\n",
    "        #y is the conditioning\n",
    "        vc2p = self.vconv(x_v)\n",
    "        vcf, vcg = torch.split(vc2p,self.out_channels,dim=1)\n",
    "        y = self.cond_embed(y)\n",
    "        yf,yg = torch.split(y,self.out_channels,dim=1)\n",
    "        #broadcast conditioning vector to all pixels\n",
    "        fv = F.tanh(vcf + yf[:,:,None,None] )\n",
    "        gv = F.sigmoid(vcg + yg[:,:,None,None] )\n",
    "        vout = fv * gv\n",
    "        \n",
    "        \n",
    "        hc2p = self.hconv(x_h) + self.v2h_conv1x1(vc2p)\n",
    "        hcf, hcg = torch.split(hc2p,self.out_channels,dim=1)\n",
    "        fh = F.tanh(hcf + yf[:,:,None,None] )\n",
    "        gh = F.sigmoid(hcg + yg[:,:,None,None] )\n",
    "        hout = fh * gh\n",
    "        if self.skip_cxn:\n",
    "            hout  = self.skip_conv1x1(hout) + x_h\n",
    "            \n",
    "        return vout,hout\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gl = GatedLayer(1,8,5,\"A\",3,skip_cxn=True)\n",
    "\n",
    "# x = torch.randint(low=0,high=255,size=(8,1,32,32))\n",
    "\n",
    "# y = torch.randint(low=0,high=512,size=(8,3))\n",
    "\n",
    "# v,h = gl(x,x,y)\n",
    "\n",
    "# gl2 = GatedLayer(8,8,5,\"B\",3,skip_cxn=True)\n",
    "\n",
    "# gl2(v,h,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedPixelCNN(nn.Module):\n",
    "    def __init__(self,num_layers=15, filt_size=5,in_channels=1,\n",
    "                 num_fm=16,cond_size=3,im_range=256):\n",
    "        \n",
    "        super(GatedPixelCNN,self).__init__()\n",
    "        self.first_layer = GatedLayer(in_channels=in_channels,out_channels=num_fm,\n",
    "                                 filter_size=filt_size,mask_type=\"A\",cond_size=3,skip_cxn=True)\n",
    "        layers = []\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.append(GatedLayer(in_channels=num_fm,out_channels=num_fm,\n",
    "                                 filter_size=filt_size,mask_type=\"B\",cond_size=3,skip_cxn=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "           \n",
    "        self.last_layer = nn.Conv2d(num_fm,im_range,1)\n",
    "            \n",
    "    def forward(self,x,y):\n",
    "        xv,xh = self.first_layer(x,x,y)\n",
    "        for layer in self.layers:\n",
    "            xv,xh = layer(xv,xh,y)\n",
    "        return self.last_layer(xh)\n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(low=0,high=512,size=(8,1,21,21))\n",
    "\n",
    "y = torch.randint(low=0,high=512,size=(8,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpcnn = GatedPixelCNN(im_range=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpcnn(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358.278204346\n",
      "291.500866699\n",
      "236.657679749\n",
      "189.84723053\n",
      "148.577581787\n"
     ]
    }
   ],
   "source": [
    "gpcnn = GatedPixelCNN(im_range=512).cuda()\n",
    "\n",
    "h,w = 21,21\n",
    "\n",
    "opt = Adam(gpcnn.parameters(),lr=0.001)\n",
    "xs = torch.randint(low=0,high=512,size=(10,8,1,21,21)).cuda()\n",
    "ys = torch.randint(low=0,high=512,size=(10,8,3)).cuda()\n",
    "\n",
    "for epoch in range(5):\n",
    "    losses = []\n",
    "    for i in range(10):\n",
    "        x = xs[i]\n",
    "        y = ys[i]\n",
    "\n",
    "        opt.zero_grad()\n",
    "        x_tild = gpcnn(x,y)\n",
    "        loss= F.cross_entropy(x_tild,x.long().squeeze())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(float(loss.data))\n",
    "    print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1e09d5b400>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1e09dac208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = torch.zeros((1,1,h,w)).cuda()\n",
    "y = ys[0][0][None,:]\n",
    "for i in range(h):\n",
    "    for j in range(w):\n",
    "        # calc unnormalized p(x_ij|x<) logits\n",
    "        pxij_logit = gpcnn(sample,y)[0,:,i,j]\n",
    "        \n",
    "        # compute normalized p(x_ij|x<) \n",
    "        pxij = F.softmax(pxij_logit,dim=0)\n",
    "        # sample from p(x_ij|x<)\n",
    "        sample[:,0,i,j] = torch.multinomial(pxij,1)\n",
    "        \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sample[0,0],cmap=\"jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
